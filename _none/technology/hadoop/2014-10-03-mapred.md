---
layout: post
title:  "MapReduce 编程"
date:   2014-10-03 19:56:52
categories: hadoop
---

MapReduce 分为四个阶段：map、combine、partition、reduce
四个阶段分别对应四种不同的类：Mapper、Combiner、Partitioner、Reducer

## Mapper
map 任务的输入为一个 \<key, value\> 对，输出为若干 \<key, value\> 对
一个典型的 Mapper 类如下：

```java
import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class WordCountMapper extends Mapper<Object, Text, Text, IntWritable> {
    IntWritable one = new IntWritable(1);
    Text       word = new Text();

    public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
        StringTokenizer itr = new StringTokenizer(value.toString());
    
        while (itr.hasMoreTokens()) {
            word.set(itr.nextToken());
            context.write(word, one);
        }   
    }   
}
```
首先，每个 Mapper 类都继承自 Mapper\<K1, V1, K2, V2\> 基类，并且都包含一个 map(K1 key, V1 value, Context context) 方法

其中，K1, V1 分别为 Mapper 输入键值对的类型，K2, V2 分别为 Mapper 输出到 context 键值对的类型

MapReduce 计算框架中，实现了 Writable 接口的类可以是值，而实现了 WritableComparable 既可以是键也可以是值。而 WritableComparable 接口是 Writable 和 java.lang.Comparable\<T\> 接口的组合。对于键而言，我们需要这个比较，因为他们将在 Reduce 阶段进行排序

Hadoop 自带的预定义实现了 WritableComparable 接口的类有：

|                  |                                |
| ---------------- | ------------------------------ |
| BooleanWritable  |      标准布尔变量的封装        |
| ByteWritable     |      单字节数的封装            |
| DoubleWritable   |      双字节数的封装            |
| FloatWritable    |      浮点数的封装              |
| IntWritable      |      整数的封装                |
| LongWritable     |      Long 的封装               |
| Text             |      使用 UTF8 格式的文本封装  |
| NullWritable     |      无键值时的占位符          |

当然你也可以自己实现自己的 WritableComparable 类，要求包含：readFields、write、compareTo 三个方法

在 map 方法里，对输入的 key, value 进行一系列的处理，得到的若干 \<K2, V2\> 通过 context.write(K2, V2) 来将结果抛出去，从而实现 map 阶段的功能

Hadoop 自带一些非常常用的预定义 Mapper 实现

|                        |                                                                                           |
| ---------------------- | ----------------------------------------------------------------------------------------- |
| IdentityMapper\<K, V\> | 实现 Mapper\<K, V, K, V\>，将输入直接映射到输出                                            |
| InverseMapper\<K, V\>  | 实现 Mapper\<K, V, V, K\>，反转键/值对                                                     |
| RegexMapper\<K\>       | 实现 Mapper\<K, Text, Text, LongWritable\>，为每个常规表达式的匹配项生成一个 (match, 1) 对 |
| TokenCountMapper\<K\>  | 实现 Mapper\<K, Text, Text, LongWritable\>，当输入的值为分词时，生成一个 (token, 1) 对     |

## Reducer
map 阶段结束之后所有 Mapper 都产生了自己的一系列 \<K, V\> 对，放在各自的 Context 里面
然后所有的 Mapper 的这些 \<K, V\> 都会被打乱重新洗牌 ( Partitioner )，默认的 Partitioner 方法就是 HashPartitioner
也就是对 Mapper 结果中的 K 按照 Reducer 的个数进行求 Hash，然后相同结果的键值对 \<K, V\> 会按照 K 进行排序然后被送入相同的 Reducer 进行处理

一个典型的 Reducer 类如下：

```java
import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
    IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
        int sum = 0;

        for (IntWritable val : values) {
            sum += val.get();
        }

        result.set(sum);
        context.write(key, result);
    }
}
```
首先，每个 Reducer 类都继承自 Reducer\<K1, V1, K2, V2\> 基类，并且都包含一个 reduce(K1 key, Iterable\<V1\> values, Context context) 方法

其中，K1, V1 分别为 Reducer 输入键值对的类型，K2, V2 分别为 Reducer 输出到 context 键值对的类型

虽然同一个 Reducer 任务可以有不同的 key (Hash 值一样) ，但是在实现 Reducer 类的时候，其所包含的 reduce 方法中的第一个参数 key 是唯一的，而非列表。第二个参数 values 则是一个包含这个 key 所对应的所有的 map 计算输出结果的列表

reduce 最终将计算结果 (一个 \<key, value\> 对) 写入到 context 中，最终同一个 Reduce 任务的 context 会被写入到同一个文件里面

Hadoop 自带一些非常常用的预定义 Reducer 实现

|                         |                                                                                           |
| ----------------------- | ----------------------------------------------------------------------------------------- |
| IdentityReducer\<K, V\> | 实现 Reducer\<K,V, K, V\>，将输入直接映射到输出                                           |
| LongSumReducer\<K\>     | 实现 \<K, LongWritable, K, LongWritable\>，计算与给定键相对应的所有值的和                 |

## Partitioner
前面介绍过，map 任务处理之后会有一个 partition 阶段，该阶段会将 map 的输出结果按照 key 进行 patition，同样的结果会交给相同的 reducer 进行处理

默认的 partition 方式是 HashPartitioner，也就是直接将 key 按照 reducer 的任务数进行 hash 求值

默认的 HashPartitioner 有时候不能满足要求，或者会造成分桶不均（某个 reducer 任务分到的 key 太多），所以就有了自定义 Partitioner 的需求

一个典型的 Partitioner 类实现如下：

```java
public class XXXPartitioner extends Partitioner<K, V> {
    public int getPartition(K key, V value, int numPartitions) {
        return Key % numPartitions;
    }
}
```

每一个 Partitioner 类都继承自 Partitioner 基类，都有一个 getPartition 方法

其中 getPartition 方法包含三个参数：map 阶段输出的 key，相应的 value，partition 的个数（reducer 的个数）

最终 getParttion 方法根据这些参数算出一个 int 值，则相应的 key 的 map 输出会分配给对应编号的 Reducer

## Combiner
Combiner 往往称之为 "本地 Reducer"，它的作用就是在 map 任务计算之后，partition 之前，先自己对结果进行一次 "reduce"，从而可以减少后续的网络数据传输量，提高 Reducer 效率

一个 Combiner 类同 Reducer 类是一样的

## 输入/输出
MapReduce 任务的输入为一个或多个 HDFS 文件，MapReduce 会根据 map 任务的数量来对所有的输入进行分片，然后分别提交给每个 map 任务进行处理

分片的大小最佳应该为 HDFS 文件系统的分片大小（默认为 64M），这样情况下就不需要在节点间传输数据了，所以要合理的设置 map 任务的数量

MapReduce 任务的输出为指定文件夹中的若干个输出文件（数量取决于 reduce 任务的数量）

典型的 MapReduce 程序结构如下：

```java
public class WordCount {
    public static void main(String[] args) {
        JobClient client = new JobClient();
        JobConf     conf = new JobConf(WordCount.class);

        FileInputFormat.addInputPath(conf,   new Path(args[0]));    // 输入文件
        FileOutputFormat.setOutputPath(conf, new Path(args[1]));    // 输出文件夹

        // 输入文件如何映射到 Mapper 的输入 <key, value> 对，由 setInputFormat 方法来确定

        conf.setOutputKeyClass(Text.class);             // 输出结果的 key 的类型
        conf.setOutputValueClass(LongWritable.class);   // 输出结果的 value 的类型

        // 输出文件如何将 Reducer 的一系列输出 <key, value> 对写入到文件，由 setOutputFormat 方法来确定

        conf.setMapperClass(TokenCountMapper.class);    // 设置 Mapper 
        conf.setCombinerClass(LongSumReducer.class);    // 设置 Combiner
        conf.setReducerClass(LongSumReducer.class);     // 设置 Reducer

        client.setConf(conf);

        try {
            JobClient.runJob(conf); 
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}
```

[jekyll-gh]: https://github.com/jekyll/jekyll
[jekyll]:    http://jekyllrb.com

