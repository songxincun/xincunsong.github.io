<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>MapReduce 编程</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="记录思绪的点点滴滴">
    <meta name="baidu-site-verification" content="CibWhF6Kxz" />
    <meta name="google-site-verification" content="pi6a8hODkaYkCKbWh0HqCs2h1OFNs7yklG8wW-VC9Fs" />
    <link rel="canonical" href="http://yourdomain.com/blog/2014/10/04/mapred">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">

</head>


    <body>

    <header class="site-header">

  <div class="wrap">

    <a class="site-title" href="/">宋新村的博客</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <div class="trigger">
        
          <a class="page-link" href="/about/">About</a>
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>MapReduce 编程</h1>
    <p class="meta">Oct 4, 2014</p>
  </header>

  <article class="post-content">
  <p>MapReduce 分为四个阶段：map、combine、partition、reduce<br>
四个阶段分别对应四种不同的类：Mapper、Combiner、Partitioner、Reducer</p>

<h2>Mapper</h2>

<p>map 任务的输入为一个 &lt;key, value&gt; 对，输出为若干 &lt;key, value&gt; 对<br>
一个典型的 Mapper 类如下：</p>
<div class="highlight"><pre><code class="language-java" data-lang="java"><span class="kn">import</span> <span class="nn">java.io.IOException</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">java.util.StringTokenizer</span><span class="o">;</span>

<span class="kn">import</span> <span class="nn">org.apache.hadoop.io.IntWritable</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.hadoop.io.Text</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.hadoop.mapreduce.Mapper</span><span class="o">;</span>

<span class="kd">public</span> <span class="kd">class</span> <span class="nc">WordCountMapper</span> <span class="kd">extends</span> <span class="n">Mapper</span><span class="o">&lt;</span><span class="n">Object</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">IntWritable</span><span class="o">&gt;</span> <span class="o">{</span>
    <span class="n">IntWritable</span> <span class="n">one</span> <span class="o">=</span> <span class="k">new</span> <span class="nf">IntWritable</span><span class="o">(</span><span class="mi">1</span><span class="o">);</span>
    <span class="n">Text</span>       <span class="n">word</span> <span class="o">=</span> <span class="k">new</span> <span class="nf">Text</span><span class="o">();</span>

    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">map</span><span class="o">(</span><span class="n">Object</span> <span class="n">key</span><span class="o">,</span> <span class="n">Text</span> <span class="n">value</span><span class="o">,</span> <span class="n">Context</span> <span class="n">context</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">IOException</span><span class="o">,</span> <span class="n">InterruptedException</span> <span class="o">{</span>
        <span class="n">StringTokenizer</span> <span class="n">itr</span> <span class="o">=</span> <span class="k">new</span> <span class="nf">StringTokenizer</span><span class="o">(</span><span class="n">value</span><span class="o">.</span><span class="na">toString</span><span class="o">());</span>

        <span class="k">while</span> <span class="o">(</span><span class="n">itr</span><span class="o">.</span><span class="na">hasMoreTokens</span><span class="o">())</span> <span class="o">{</span>
            <span class="n">word</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">itr</span><span class="o">.</span><span class="na">nextToken</span><span class="o">());</span>
            <span class="n">context</span><span class="o">.</span><span class="na">write</span><span class="o">(</span><span class="n">word</span><span class="o">,</span> <span class="n">one</span><span class="o">);</span>
        <span class="o">}</span>   
    <span class="o">}</span>   
<span class="o">}</span>
</code></pre></div>
<p>首先，每个 Mapper 类都继承自 Mapper&lt;K1, V1, K2, V2&gt; 基类，并且都包含一个 map(K1 key, V1 value, Context context) 方法</p>

<p>其中，K1, V1 分别为 Mapper 输入键值对的类型，K2, V2 分别为 Mapper 输出到 context 键值对的类型</p>

<p>MapReduce 计算框架中，实现了 Writable 接口的类可以是值，而实现了 WritableComparable 既可以是键也可以是值。而 WritableComparable 接口是 Writable 和 java.lang.Comparable&lt;T&gt; 接口的组合。对于键而言，我们需要这个比较，因为他们将在 Reduce 阶段进行排序</p>

<p>Hadoop 自带的预定义实现了 WritableComparable 接口的类有：</p>

<table><thead>
<tr>
<th></th>
<th></th>
</tr>
</thead><tbody>
<tr>
<td>BooleanWritable</td>
<td>标准布尔变量的封装</td>
</tr>
<tr>
<td>ByteWritable</td>
<td>单字节数的封装</td>
</tr>
<tr>
<td>DoubleWritable</td>
<td>双字节数的封装</td>
</tr>
<tr>
<td>FloatWritable</td>
<td>浮点数的封装</td>
</tr>
<tr>
<td>IntWritable</td>
<td>整数的封装</td>
</tr>
<tr>
<td>LongWritable</td>
<td>Long 的封装</td>
</tr>
<tr>
<td>Text</td>
<td>使用 UTF8 格式的文本封装</td>
</tr>
<tr>
<td>NullWritable</td>
<td>无键值时的占位符</td>
</tr>
</tbody></table>

<p>当然你也可以自己实现自己的 WritableComparable 类，要求包含：readFields、write、compareTo 三个方法</p>

<p>在 map 方法里，对输入的 key, value 进行一系列的处理，得到的若干 &lt;K2, V2&gt; 通过 context.write(K2, V2) 来将结果抛出去，从而实现 map 阶段的功能</p>

<p>Hadoop 自带一些非常常用的预定义 Mapper 实现</p>

<table><thead>
<tr>
<th></th>
<th></th>
</tr>
</thead><tbody>
<tr>
<td>IdentityMapper&lt;K, V&gt;</td>
<td>实现 Mapper&lt;K, V, K, V&gt;，将输入直接映射到输出</td>
</tr>
<tr>
<td>InverseMapper&lt;K, V&gt;</td>
<td>实现 Mapper&lt;K, V, V, K&gt;，反转键/值对</td>
</tr>
<tr>
<td>RegexMapper&lt;K&gt;</td>
<td>实现 Mapper&lt;K, Text, Text, LongWritable&gt;，为每个常规表达式的匹配项生成一个 (match, 1) 对</td>
</tr>
<tr>
<td>TokenCountMapper&lt;K&gt;</td>
<td>实现 Mapper&lt;K, Text, Text, LongWritable&gt;，当输入的值为分词时，生成一个 (token, 1) 对</td>
</tr>
</tbody></table>

<h2>Reducer</h2>

<p>map 阶段结束之后所有 Mapper 都产生了自己的一系列 &lt;K, V&gt; 对，放在各自的 Context 里面<br>
然后所有的 Mapper 的这些 &lt;K, V&gt; 都会被打乱重新洗牌 ( Partitioner )，默认的 Partitioner 方法就是 HashPartitioner<br>
也就是对 Mapper 结果中的 K 按照 Reducer 的个数进行求 Hash，然后相同结果的键值对 &lt;K, V&gt; 会按照 K 进行排序然后被送入相同的 Reducer 进行处理</p>

<p>一个典型的 Reducer 类如下：</p>
<div class="highlight"><pre><code class="language-java" data-lang="java"><span class="kn">import</span> <span class="nn">java.io.IOException</span><span class="o">;</span>

<span class="kn">import</span> <span class="nn">org.apache.hadoop.io.IntWritable</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.hadoop.io.Text</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.hadoop.mapreduce.Reducer</span><span class="o">;</span>

<span class="kd">public</span> <span class="kd">class</span> <span class="nc">WordCountReducer</span> <span class="kd">extends</span> <span class="n">Reducer</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">IntWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">IntWritable</span><span class="o">&gt;</span> <span class="o">{</span>
    <span class="n">IntWritable</span> <span class="n">result</span> <span class="o">=</span> <span class="k">new</span> <span class="nf">IntWritable</span><span class="o">();</span>

    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">reduce</span><span class="o">(</span><span class="n">Text</span> <span class="n">key</span><span class="o">,</span> <span class="n">Iterable</span><span class="o">&lt;</span><span class="n">IntWritable</span><span class="o">&gt;</span> <span class="n">values</span><span class="o">,</span> <span class="n">Context</span> <span class="n">context</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">IOException</span><span class="o">,</span> <span class="n">InterruptedException</span> <span class="o">{</span>
        <span class="kt">int</span> <span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span>

        <span class="k">for</span> <span class="o">(</span><span class="n">IntWritable</span> <span class="n">val</span> <span class="o">:</span> <span class="n">values</span><span class="o">)</span> <span class="o">{</span>
            <span class="n">sum</span> <span class="o">+=</span> <span class="n">val</span><span class="o">.</span><span class="na">get</span><span class="o">();</span>
        <span class="o">}</span>

        <span class="n">result</span><span class="o">.</span><span class="na">set</span><span class="o">(</span><span class="n">sum</span><span class="o">);</span>
        <span class="n">context</span><span class="o">.</span><span class="na">write</span><span class="o">(</span><span class="n">key</span><span class="o">,</span> <span class="n">result</span><span class="o">);</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div>
<p>首先，每个 Reducer 类都继承自 Reducer&lt;K1, V1, K2, V2&gt; 基类，并且都包含一个 reduce(K1 key, Iterable&lt;V1&gt; values, Context context) 方法</p>

<p>其中，K1, V1 分别为 Reducer 输入键值对的类型，K2, V2 分别为 Reducer 输出到 context 键值对的类型</p>

<p>虽然同一个 Reducer 任务可以有不同的 key (Hash 值一样) ，但是在实现 Reducer 类的时候，其所包含的 reduce 方法中的第一个参数 key 是唯一的，而非列表。第二个参数 values 则是一个包含这个 key 所对应的所有的 map 计算输出结果的列表</p>

<p>reduce 最终将计算结果 (一个 &lt;key, value&gt; 对) 写入到 context 中，最终同一个 Reduce 任务的 context 会被写入到同一个文件里面</p>

<p>Hadoop 自带一些非常常用的预定义 Reducer 实现</p>

<table><thead>
<tr>
<th></th>
<th></th>
</tr>
</thead><tbody>
<tr>
<td>IdentityReducer&lt;K, V&gt;</td>
<td>实现 Reducer&lt;K,V, K, V&gt;，将输入直接映射到输出</td>
</tr>
<tr>
<td>LongSumReducer&lt;K&gt;</td>
<td>实现 &lt;K, LongWritable, K, LongWritable&gt;，计算与给定键相对应的所有值的和</td>
</tr>
</tbody></table>

<h2>Partitioner</h2>

<p>前面介绍过，map 任务处理之后会有一个 partition 阶段，该阶段会将 map 的输出结果按照 key 进行 patition，同样的结果会交给相同的 reducer 进行处理</p>

<p>默认的 partition 方式是 HashPartitioner，也就是直接将 key 按照 reducer 的任务数进行 hash 求值</p>

<p>默认的 HashPartitioner 有时候不能满足要求，或者会造成分桶不均（某个 reducer 任务分到的 key 太多），所以就有了自定义 Partitioner 的需求</p>

<p>一个典型的 Partitioner 类实现如下：</p>
<div class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">class</span> <span class="nc">XXXPartitioner</span> <span class="kd">extends</span> <span class="n">Partitioner</span><span class="o">&lt;</span><span class="n">K</span><span class="o">,</span> <span class="n">V</span><span class="o">&gt;</span> <span class="o">{</span>
    <span class="kd">public</span> <span class="kt">int</span> <span class="nf">getPartition</span><span class="o">(</span><span class="n">K</span> <span class="n">key</span><span class="o">,</span> <span class="n">V</span> <span class="n">value</span><span class="o">,</span> <span class="kt">int</span> <span class="n">numPartitions</span><span class="o">)</span> <span class="o">{</span>
        <span class="k">return</span> <span class="n">Key</span> <span class="o">%</span> <span class="n">numPartitions</span><span class="o">;</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div>
<p>每一个 Partitioner 类都继承自 Partitioner 基类，都有一个 getPartition 方法</p>

<p>其中 getPartition 方法包含三个参数：map 阶段输出的 key，相应的 value，partition 的个数（reducer 的个数）</p>

<p>最终 getParttion 方法根据这些参数算出一个 int 值，则相应的 key 的 map 输出会分配给对应编号的 Reducer</p>

<h2>Combiner</h2>

<p>Combiner 往往称之为 &quot;本地 Reducer&quot;，它的作用就是在 map 任务计算之后，partition 之前，先自己对结果进行一次 &quot;reduce&quot;，从而可以减少后续的网络数据传输量，提高 Reducer 效率</p>

<p>一个 Combiner 类同 Reducer 类是一样的</p>

<h2>输入/输出</h2>

<p>MapReduce 任务的输入为一个或多个 HDFS 文件，MapReduce 会根据 map 任务的数量来对所有的输入进行分片，然后分别提交给每个 map 任务进行处理</p>

<p>分片的大小最佳应该为 HDFS 文件系统的分片大小（默认为 64M），这样情况下就不需要在节点间传输数据了，所以要合理的设置 map 任务的数量</p>

<p>MapReduce 任务的输出为指定文件夹中的若干个输出文件（数量取决于 reduce 任务的数量）</p>

<p>典型的 MapReduce 程序结构如下：</p>
<div class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">class</span> <span class="nc">WordCount</span> <span class="o">{</span>
    <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">JobClient</span> <span class="n">client</span> <span class="o">=</span> <span class="k">new</span> <span class="nf">JobClient</span><span class="o">();</span>
        <span class="n">JobConf</span>     <span class="n">conf</span> <span class="o">=</span> <span class="k">new</span> <span class="nf">JobConf</span><span class="o">(</span><span class="n">WordCount</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>

        <span class="n">FileInputFormat</span><span class="o">.</span><span class="na">addInputPath</span><span class="o">(</span><span class="n">conf</span><span class="o">,</span>   <span class="k">new</span> <span class="nf">Path</span><span class="o">(</span><span class="n">args</span><span class="o">[</span><span class="mi">0</span><span class="o">]));</span>    <span class="c1">// 输入文件</span>
        <span class="n">FileOutputFormat</span><span class="o">.</span><span class="na">setOutputPath</span><span class="o">(</span><span class="n">conf</span><span class="o">,</span> <span class="k">new</span> <span class="nf">Path</span><span class="o">(</span><span class="n">args</span><span class="o">[</span><span class="mi">1</span><span class="o">]));</span>    <span class="c1">// 输出文件夹</span>

        <span class="c1">// 输入文件如何映射到 Mapper 的输入 &lt;key, value&gt; 对，由 setInputFormat 方法来确定</span>

        <span class="n">conf</span><span class="o">.</span><span class="na">setOutputKeyClass</span><span class="o">(</span><span class="n">Text</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>             <span class="c1">// 输出结果的 key 的类型</span>
        <span class="n">conf</span><span class="o">.</span><span class="na">setOutputValueClass</span><span class="o">(</span><span class="n">LongWritable</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>   <span class="c1">// 输出结果的 value 的类型</span>

        <span class="c1">// 输出文件如何将 Reducer 的一系列输出 &lt;key, value&gt; 对写入到文件，由 setOutputFormat 方法来确定</span>

        <span class="n">conf</span><span class="o">.</span><span class="na">setMapperClass</span><span class="o">(</span><span class="n">TokenCountMapper</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>    <span class="c1">// 设置 Mapper </span>
        <span class="n">conf</span><span class="o">.</span><span class="na">setCombinerClass</span><span class="o">(</span><span class="n">LongSumReducer</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>    <span class="c1">// 设置 Combiner</span>
        <span class="n">conf</span><span class="o">.</span><span class="na">setReducerClass</span><span class="o">(</span><span class="n">LongSumReducer</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>     <span class="c1">// 设置 Reducer</span>

        <span class="n">client</span><span class="o">.</span><span class="na">setConf</span><span class="o">(</span><span class="n">conf</span><span class="o">);</span>

        <span class="k">try</span> <span class="o">{</span>
            <span class="n">JobClient</span><span class="o">.</span><span class="na">runJob</span><span class="o">(</span><span class="n">conf</span><span class="o">);</span> 
        <span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="n">Exception</span> <span class="n">e</span><span class="o">)</span> <span class="o">{</span>
            <span class="n">e</span><span class="o">.</span><span class="na">printStackTrace</span><span class="o">();</span>
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div>
  </article>

</div>

<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'songxincun'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); 
      
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
     
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>

<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'songxincun'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); 

        s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';

        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>



      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <h2 class="footer-heading">宋新村的博客</h2>

    <div class="footer-col-1 column">
      <ul>
        <li>宋新村的博客</li>
        <li><a href="mailto:your-email@domain.com">your-email@domain.com</a></li>
      </ul>
    </div>

    <div class="footer-col-2 column">
      <ul>
        <li>
          <a href="https://github.com/songxincun">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">songxincun</span>
          </a>
        </li>
        
      </ul>
    </div>

    <div class="footer-col-3 column">
      <p class="text">记录思绪的点点滴滴</p>
    </div>

  </div>

</footer>


    </body>
</html>